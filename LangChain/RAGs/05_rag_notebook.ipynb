{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_TRACING_V2=\"false\"\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY=\"api_key\"\n",
    "LANGCHAIN_PROJECT=\"Zero2LLMOps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = LANGCHAIN_TRACING_V2\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = LANGCHAIN_ENDPOINT\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
    "os.environ['LANGCHAIN_PROJECT'] = LANGCHAIN_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path=\"./transformersPaper.pdf\"\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OllamaEmbeddings(\n",
    "#     model=\"nomic-embed-text:latest\"\n",
    "# )\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = model\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"llama3.1:latest\",\n",
    "#     temperature=0\n",
    "# )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain =(\n",
    "    RunnableParallel({\"context\": retriver | format_docs, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors of the Transformer paper are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Lukasz Kaiser, and Aidan N. Gomez, among others. Their work proposed a novel model architecture relying entirely on attention mechanisms. The paper is titled \"Attention is All You Need,\" published in 2017.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is the Author of Transformer paper ??\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What is the Author of Transformer paper ??\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Who are the authors of the original Transformer paper titled \"Attention is All You Need\"?',\n",
       " '2. What contributions did each author make to the Transformer paper?',\n",
       " '3. When was the Transformer paper published and in which conference or journal?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriver, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer paper titled \"Attention is All You Need\" was published in June 2017 at the Neural Information Processing Systems (NeurIPS) conference.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: 3. When was the Transformer paper published and in which conference or journal?\\nAnswer: The Transformer paper titled \"Attention is All You Need\" was published in June 2017 at the Neural Information Processing Systems (NeurIPS) conference.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Question: 1. Who are the authors of the original Transformer paper titled \"Attention is All You Need\"?\n",
      "Answer: The authors of the original Transformer paper titled \"Attention is All You Need\" are:\n",
      "\n",
      "1. Ashish Vaswani\n",
      "2. Noam Shazeer\n",
      "3. Niki Parmar\n",
      "4. Jakob Uszkoreit\n",
      "5. Llion Jones\n",
      "6. Aidan N. Gomez\n",
      "7. Łukasz Kaiser\n",
      "8. Illia Polosukhin\n",
      "---\n",
      "Question: 2. What contributions did each author make to the Transformer paper?\n",
      "Answer: The contributions of each author to the Transformer paper titled \"Attention is All You Need\" are as follows:\n",
      "\n",
      "1. **Ashish Vaswani**: Designed and implemented the first Transformer models and was involved in nearly every aspect of the work.\n",
      "\n",
      "2. **Noam Shazeer**: Proposed scaled dot-product attention, multi-head attention, and the parameter-free position representation, contributing to nearly every detail of the project.\n",
      "\n",
      "3. **Niki Parmar**: Designed, implemented, tuned, and evaluated numerous model variants in the original codebase and tensor2tensor.\n",
      "\n",
      "4. **Jakob Uszkoreit**: Proposed replacing RNNs with self-attention and initiated the effort to evaluate this idea.\n",
      "\n",
      "5. **Llion Jones**: Experimented with novel model variants, was responsible for the initial codebase, and worked on efficient inference and visualizations.\n",
      "\n",
      "6. **Aidan N. Gomez**: Contributed to designing various parts of the model, alongside his colleagues.\n",
      "\n",
      "7. **Łukasz Kaiser**: Spent significant time designing different aspects of the model architecture.\n",
      "\n",
      "8. **Illia Polosukhin**: Worked with Ashish Vaswani in designing and implementing the first Transformer models and contributed to various parts of the project.\n",
      "\n",
      "The authors are noted to have made equal contributions, and the listing order was randomized.\n",
      "---\n",
      "Question: 3. When was the Transformer paper published and in which conference or journal?\n",
      "Answer: The Transformer paper titled \"Attention is All You Need\" was published in June 2017 at the Neural Information Processing Systems (NeurIPS) conference.\n"
     ]
    }
   ],
   "source": [
    "print(q_a_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAG Fusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RRF Explanation\n",
    "\n",
    "#### Question Rankings\n",
    "\n",
    "- **Question A**: \n",
    "  1. Doc1\n",
    "  2. Doc4\n",
    "  3. Doc3\n",
    "  4. Doc2\n",
    "\n",
    "- **Question B**: \n",
    "  1. Doc3\n",
    "  2. Doc1\n",
    "  3. Doc2\n",
    "  4. Doc4\n",
    "\n",
    "- **Question C**: \n",
    "  1. Doc4\n",
    "  2. Doc3\n",
    "  3. Doc1\n",
    "  4. Doc2\n",
    "\n",
    "### Rank Positions\n",
    "\n",
    "- **Doc1**:\n",
    "  - Question A rank: 1\n",
    "  - Question B rank: 2\n",
    "  - Question C rank: 3\n",
    "\n",
    "- **Doc2**:\n",
    "  - Question A rank: 4\n",
    "  - Question B rank: 3\n",
    "  - Question C rank: 4\n",
    "\n",
    "- **Doc3**:\n",
    "  - Question A rank: 3\n",
    "  - Question B rank: 1\n",
    "  - Question C rank: 2\n",
    "\n",
    "- **Doc4**:\n",
    "  - Question A rank: 2\n",
    "  - Question B rank: 4\n",
    "  - Question C rank: 1\n",
    "\n",
    "### Reciprocal Rank Fusion Calculation\n",
    "\n",
    "Using `k = 60`:\n",
    "\n",
    "#### Doc1\n",
    "- Reciprocal Rank (Question A): `1 / (60 + 1) = 1 / 61`\n",
    "- Reciprocal Rank (Question B): `1 / (60 + 2) = 1 / 62`\n",
    "- Reciprocal Rank (Question C): `1 / (60 + 3) = 1 / 63`\n",
    "- **RRF(Doc1)**: `1 / 61 + 1 / 62 + 1 / 63 ≈ 0.0487`\n",
    "\n",
    "#### Doc2\n",
    "- Reciprocal Rank (Question A): `1 / (60 + 4) = 1 / 64`\n",
    "- Reciprocal Rank (Question B): `1 / (60 + 3) = 1 / 63`\n",
    "- Reciprocal Rank (Question C): `1 / (60 + 4) = 1 / 64`\n",
    "- **RRF(Doc2)**: `1 / 64 + 1 / 63 + 1 / 64 ≈ 0.0469`\n",
    "\n",
    "#### Doc3\n",
    "- Reciprocal Rank (Question A): `1 / (60 + 3) = 1 / 63`\n",
    "- Reciprocal Rank (Question B): `1 / (60 + 1) = 1 / 61`\n",
    "- Reciprocal Rank (Question C): `1 / (60 + 2) = 1 / 62`\n",
    "- **RRF(Doc3)**: `1 / 63 + 1 / 61 + 1 / 62 ≈ 0.0487`\n",
    "\n",
    "#### Doc4\n",
    "- Reciprocal Rank (Question A): `1 / (60 + 2) = 1 / 62`\n",
    "- Reciprocal Rank (Question B): `1 / (60 + 4) = 1 / 64`\n",
    "- Reciprocal Rank (Question C): `1 / (60 + 1) = 1 / 61`\n",
    "- **RRF(Doc4)**: `1 / 62 + 1 / 64 + 1 / 61 ≈ 0.0484`\n",
    "\n",
    "### Final Ranking\n",
    "\n",
    "Based on the RRF scores:\n",
    "\n",
    "1. **Doc1**: `≈ 0.0487`\n",
    "2. **Doc3**: `≈ 0.0487`\n",
    "3. **Doc4**: `≈ 0.0484`\n",
    "4. **Doc2**: `≈ 0.0469`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriver.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
