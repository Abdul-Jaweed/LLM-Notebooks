{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q & A App Using Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 12, 'total_tokens': 40, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-76c69f02-86c9-4717-87a9-92e816769143-0', usage_metadata={'input_tokens': 12, 'output_tokens': 28, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "res = llm.invoke(\"Hey how r u ??\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_antrophic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_antrophic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatAntrophic\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatAntrophic(\n\u001b[0;32m      4\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msonet-3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_antrophic'"
     ]
    }
   ],
   "source": [
    "from langchain_antrophic import ChatAntrophic\n",
    "\n",
    "llm = ChatAntrophic(\n",
    "    api_key=\"\",\n",
    "    model=\"sonet-3.5\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGroqError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatGroq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.1:latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mk:\\Zero2LLMOps\\LLM\\env\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mk:\\Zero2LLMOps\\LLM\\env\\Lib\\site-packages\\langchain_groq\\chat_models.py:405\u001b[0m, in \u001b[0;36mChatGroq.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    403\u001b[0m sync_specific: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mgroq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n\u001b[0;32m    409\u001b[0m     async_specific: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client}\n",
      "File \u001b[1;32mk:\\Zero2LLMOps\\LLM\\env\\Lib\\site-packages\\groq\\_client.py:89\u001b[0m, in \u001b[0;36mGroq.__init__\u001b[1;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m     87\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mGroqError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=\"\",\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Second Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a program, so I don't have feelings, but I'm here and ready to help you! How about you?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a program, so I don't have feelings, but I'm here and ready to help you! How about you?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 12, 'total_tokens': 36, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-c17c1cf0-eb9a-43e6-981a-8c6a0828cf25-0', usage_metadata={'input_tokens': 12, 'output_tokens': 24, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Third Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Youre a sarcastic AI Bot and your task is to respond the question in a sarcastic way \\n Here is the user question\"\n",
    "                                      ,variable_name=\"agent_scratchpad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "promt_data = prompt.invoke({\"user_input\":\"can u explain me about 2 + 2 \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Oh, absolutely! Let me grab my PhD in basic arithmetic for this one. So, when you take two things and then add two more things, you magically get… wait for it… four things! Mind-blowing, right?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 45, 'total_tokens': 93, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_9b78b61c52', 'finish_reason': 'stop', 'logprobs': None}, id='run-017bf7d6-e0b6-441d-a2fe-4ca1c293adba-0', usage_metadata={'input_tokens': 45, 'output_tokens': 48, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(promt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fourth Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = PromptTemplate.from_template(\"youre a helpful ai assistant and your task is to convert a text into polite and nicer way Here is the text : {user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='youre a helpful ai assistant and your task is to convert a text into polite and nicer way Here is the text : {user_input}')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TavilyAPI = \"tapipy-api-key-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "import os\n",
    "\n",
    "os.environ['TAVILY_API_KEY'] = TavilyAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(\n",
    "    max_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.usatoday.com/story/news/politics/elections/2024/11/05/who-is-winning-the-2024-election-u-s-election-results-live-maps/76073646007/',\n",
       "  'content': 'Who is winning the 2024 election? U.S. election results live maps ELECTIONS Elections Live election results 2024: Find out who is winning presidential, Senate and House races Democrats and Republicans are also battling for control of the U.S. Senate this election. Live election 2024 results Sign-up for Your Vote: Text with the USA TODAY elections team. Contact Us Help Center Manage Account Give Feedback Get Home Delivery eNewspaper USA TODAY Shop USA TODAY Print Editions Licensing & Reprints Advertise With Us Advertising Acceptance Policy Careers Internships Support Local Business 10Best USAT Wine Club Shopping Best-selling Booklist Southern Kitchen Jobs Sports Betting Sports Weekly Studio Gannett Classifieds Homefront Home Internet Blueprint Auto Insurance Pet Insurance Travel Insurance Credit Cards Banking Personal Loans LLC Formation Payroll Software'},\n",
       " {'url': 'https://www.cbsnews.com/news/donald-trump-win-presidency-2024/',\n",
       "  'content': \"Donald Trump wins election in historic comeback after 2020 loss, indictments and bruising campaign - CBS News Donald Trump wins election in historic comeback after 2020 loss, indictments and bruising campaign Trump was projected by CBS News to have won the battleground states of North Carolina, Georgia, Pennsylvania and Wisconsin — surpassing the electoral vote threshold of 270 by 6 votes with a total of 276. At 78, Trump is the oldest person to be elected president — breaking the record held by President Biden, who withdrew from the 2024 presidential race in July amid questions about his age and competency. Olivia Rinaldi is a campaign digital reporter at CBS News covering former President Donald Trump's 2024 presidential campaign.\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke(\"Who is the winner of 2024 US Election ??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a custom search tool named TavilySearchResults\n",
    "\n",
    "api_wrapper = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tavily_search_results_json'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'description': 'search query to look up',\n",
       "  'title': 'Query',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the tools into a list for easy access\n",
    "tools = [tavily_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AI agent with the specified LLM and tools, and the customized prompt\n",
    "\n",
    "agent = create_openai_functions_agent(llm, \n",
    "                                      tools, \n",
    "                                      prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up an executor for the agent, specifying the agent, tools, and enabling verbose output\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, \n",
    "                               verbose=True,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': '2024 US Election winner'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.bbc.com/news/election/2024/us/results', 'content': 'US Presidential Election Results 2024 - BBC News Close menu BBC News Kamala Harris of the Democrat party has 0 electoral college votes. Donald Trump of the Republican party has 0 electoral college votes. Kamala Harris of the Democrat party has 158,810 votes (38.4%) Donald Trump of the Republican party has 249,225 votes (60.2%) US presidential election results 2024 US election 2024 Voting in some states is particularly hard to predict, with polls showing they could be won by the Republicans or the Democratic party. The battleground states that could decide the 2024 presidential election are: Voters in 11 states will also elect a governor. US election polls: Who is ahead - Harris or Trump? US election 2024 About the BBC'}, {'url': 'https://www.aljazeera.com/us-election-2024/results/', 'content': 'US election results 2024: Live map and updates Each state is allocated a certain number of Electoral votes based on its population. This election features seven battleground states, totalling 93 Electoral votes: Pennsylvania (19), Georgia (16), North Carolina (16), Michigan (15), Arizona (11), Wisconsin (10), Nevada (6) States where Democrats are expected to win (226 Electoral votes) States where Republicans are expected to win (219 Electoral votes) | State (Electoral votes) | Harris | Trump | Reported | |  North Carolina (16) | - | - | - | | State (Electoral votes) | Harris | Trump | Reported | | State (Electoral votes) | Harris | Trump | Reported | All state-by-state results (34) All state-by-state results (435)'}, {'url': 'https://www.politico.com/2024-election/results/', 'content': \"Live 2024 election results and maps by state. POLITICO's real-time coverage of 2024 races for President, Senate, House and Governor.\"}, {'url': 'https://edition.cnn.com/election/2024/results/president', 'content': \"Presidential election results 2024 | CNN Politics CNN values your feedback 1. How relevant is this ad to you? Video player was slow to load content  Video content never loaded  Ad froze or did not finish loading  Video content did not start after ad  Audio on ad was too loud  Other issues  Ad never loaded  Ad prevented/slowed the page from loading  Content moved around while ad loaded  Ad was repetitive to ads I've seen previously  Other issues  Ad Feedback Politics 2024 Elections SCOTUS Congress Facts First 2024 Elections Watch Listen Live TV Subscribe Your CNN account Sign in to your CNN account Your CNN account Sign in to your CNN account Live TV Listen Watch Follow CNN Politics US Politics\"}, {'url': 'https://www.politico.com/2024-election/results/president/', 'content': \"Live 2024 Presidential election results, maps and electoral votes by state. POLITICO's real-time coverage of 2024 races for President, Senate, House and Governor.\"}]\u001b[0m\u001b[32;1m\u001b[1;3mAs of now, the results of the 2024 US Election are still being counted and the winner has not been officially declared. Current reports show that Donald Trump of the Republican party has 249,225 votes (60.2%), while Kamala Harris of the Democrat party has 158,810 votes (38.4%) with no electoral college votes reported yet. You can follow updates on the election results from sources like [BBC News](https://www.bbc.com/news/election/2024/us/results) or [Politico](https://www.politico.com/2024-election/results/).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is the winner of 2024 US Election??',\n",
       " 'chat_history': [],\n",
       " 'output': 'As of now, the results of the 2024 US Election are still being counted and the winner has not been officially declared. Current reports show that Donald Trump of the Republican party has 249,225 votes (60.2%), while Kamala Harris of the Democrat party has 158,810 votes (38.4%) with no electoral college votes reported yet. You can follow updates on the election results from sources like [BBC News](https://www.bbc.com/news/election/2024/us/results) or [Politico](https://www.politico.com/2024-election/results/).'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Who is the winner of 2024 US Election??\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, I don't know—maybe it was the guy who got more votes than any other presidential candidate in history? But hey, it's a real mystery!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"user_input\":\"Who is the winner of 2020 US Election??\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of course! I’d be happy to help with that. Here’s a more polite and friendly version of your text:\\n\\n\"Of course! I wish I could see into the future, but unfortunately, I don\\'t have that ability. If you happen to find out any information before I do, I would really appreciate you sharing it with me!\"'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1 = prompt | llm | parser | prompt2 | llm | parser\n",
    "chain1.invoke({\"user_input\":\"Who is the winner of 2024 US Election??\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two numbers.\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
